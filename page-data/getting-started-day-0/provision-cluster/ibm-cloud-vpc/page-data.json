{"componentChunkName":"component---src-pages-getting-started-day-0-provision-cluster-ibm-cloud-vpc-mdx","path":"/getting-started-day-0/provision-cluster/ibm-cloud-vpc/","result":{"pageContext":{"frontmatter":{"title":"Provision an IBM Cloud cluster","tabs":["IBM Cloud VPC","IBM Cloud Classic","Multi-Cloud"]},"relativePagePath":"/getting-started-day-0/provision-cluster/ibm-cloud-vpc.mdx","titleType":"page","MdxNode":{"id":"4eb47db6-6fb8-526a-8d13-15e3b27c7192","children":[],"parent":"91e0de22-e59e-534f-a301-b5a046eede9b","internal":{"content":"---\ntitle: Provision an IBM Cloud cluster \ntabs: ['IBM Cloud VPC', 'IBM Cloud Classic', 'Multi-Cloud']\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\nSteps to provision a new <Globals name=\"ic\" />-managed cluster\nrunning on VPC infrastructure using the <Globals name=\"shortName\" />.\n\n</PageDescription>\n\nSee the following for instructions on how to\nprovision [new <Globals name=\"ic\" />-managed clusters running on classic\ninfrastructure](./ibm-cloud-on-classic)\n\n<Tabs>\n\n<Tab label=\"Private Catalog\" open=\"true\">\n\n<InlineNotification>\n\nThese steps assume the private catalog has been created and populated with the <Globals name=\"shortName\" /> tiles during \nthe [prepare the account](../prepare-account#3.-create-the-private-catalog) steps.\n\n</InlineNotification>\n\n<br />\n\n1. Log in to the <Globals name=\"ic\" /> Console.\n2. Select **Catalog** from the top menu.\n3. From the side menu, select your catalog from the drop-down list (e.g. `Team Catalog`). (**IBM Cloud catalog** should be selected initially.)\n4. Click **Private** on the side menu to see the private catalog entries\n5. Click on the **220. Cloud-Native VPC cluster** tile\n6. Enter values for the variables list provided.\n\n    | **Variable**          | **Description**                                                                               | **eg. Value**                 |\n    |-----------------------|-----------------------------------------------------------------------------------------------|-------------------------------|\n    | `ibmcloud_api_key`    | The API key from IBM Cloud Console that has ClusterAdmin access and supports service creation | `{guid API key from Console}` |\n    | `resource_group_name` | The existing resource group in the account where the cluster will be created                  | `dev-team-one`                |\n    | `region`              | The region where the cluster will be provisioned.                                             | `us-east`, `eu-gb`, etc       |\n    | `cluster_name`        | The name of the cluster that will be provisioned.                                             | `dev-team-one-iks-117-vpc`    |\n    | `vpc_zone_names`      | A comma-separated list of the VPC zones that should be used for worker nodes.                 | `us-south-1` or `us-east-1,us-east-2` |\n    | `cluster_type`        | The type of cluster into which the toolkit will be installed. The default is `OpenShift 4.5`. | `kubernetes`, `ocp3`, `ocp4`, `ocp44`, or `ocp45` |\n    | `flavor`              | The flavor of machine that should be provisioned for each worker. Defaults to `mx2.4x32`.     | `mx2.4x32`          |\n    | `cluster_worker_count`| The number of worker nodes that should be provisioned for each zone. Defaults to `3`          | `3` |\n    | `cluster_provision_cos`| Flag indicating that a new Object Storage instance should be provisioned. Defaults to `true` | `true` or `false`          |\n    | `cos_name`            | The name of the Object Storage instance (If `cluster_provision_cose` is set to `true` this value is required | `cntk-showcase-cos` |\n\n7. Check the box to accept the **Apache 2** license for the tile.\n8. Click **Install** to start the install process\n\nThis will kick off the installation of the <Globals name=\"shortName\" /> using an\nIBM Cloud Private Catalog Tile. The progress can be reviewed from the\n**Schematics** entry\n\n</Tab>\n\n<Tab label=\"Iteration Zero\">\n\n### A. Download the Iteration Zero scripts\n\n<br />\n\n1. Clone the [ibm-garage-iteration-zero](https://github.com/ibm-garage-cloud/ibm-garage-iteration-zero) Git repository to your local filesystem\n   \n    ```shell\n    git clone git@github.com:ibm-garage-cloud/ibm-garage-iteration-zero.git\n    ```\n\n### B. Configure the credentials\n\n<br />\n\n1. In a terminal, change to the `ibm-garage-iteration-zero` cloned directory\n   \n    ```shell\n    cd ibm-garage-iteration-zero\n    ```\n\n2. Copy the `credentials.template` file to a file named `credentials.properties`\n   \n    ```shell\n    cp credentials.template credentials.properties\n    ```\n\n   **Note:** `credentials.properties` is already listed in `.gitignore` to prevent the\n   private credentials from being committed to the git repository\n\n3. Update the value for the `ibmcloud.api.key` property in `credentials.properties` with your <Globals name=\"ic\" /> API key\n\n    **Note:** The API key should have been set up during [plan installation](/getting-started-0/prepare-account).\n\n### C. Configure the environment variables\n\n<br />\n\nThe settings for creating the <Globals name=\"shortName\" /> on <Globals name=\"ic\" /> are set in the `environment-ibmcloud.tfvars`\nfile in the `./terraform/settings` directory of the `ibm-garage-iteration-zero` repository.\n\nThere are a number of values that can be applied in the file, some required and some optional. Consult with\nthe following table to determine which values should be used:\n\n| **Variable**          | **Required?** | **Description**                                                                     | **eg. Value**                 |\n|-----------------------|-----|-----------------------------------------------------------------------------------------------|-------------------------------|\n| `cluster_type`        | yes | The type of cluster into which the toolkit will be installed                                  | `kubernetes`, `ocp3`, `ocp4` or `ocp44` |\n| `cluster_exists`      | yes | Flag indicating if the cluster already exists. (`false` means the cluster should be provisioned) | `false`          |\n| `vpc_cluster`         | yes | Flag indicating that the cluster has been built on VPC infrastructure. Defaults to `true`     | `true`          |\n| `name_prefix`         | no  | The prefix that should be applied for any resources that are provisioned. Defaults to `{resource_group_name}` | `dev-one`     |\n| `cluster_name`        | no  | The name of the cluster (If `cluster_exists` is set to `true` then this name should match an existing cluster). Defaults to `{prefix_name}-cluster` or `{resource_group_name}-cluster` | `dev-team-one-iks-117-vpc` |\n| `resource_group_name` | yes | Existing resource group in the account where the cluster has been created                     | `dev-team-one`                |\n| `region`              | yes | The region where the cluster has been/will be provisioned                                     | `us-east`, `eu-gb`, etc       |\n| `vpc_zone_names`      | no  | A comma-separated list of the VPC zones that should be used for worker nodes. This value is required if `cluster_exists` is set to `false` and `vpc_cluster` is set to `true` | `us-south-1` or `us-east-1,us-east-2` |\n| `provision_logdna`    | no  | Flag indicating that a new instance of LogDNA should be provisioned. Defaults to `false`      | `true` or `false`          |\n| `logdna_name`         | no  | The name of the LogDNA instance (If `provision_logdna` is set to `false` this value is used by the scripts to bind the existing LogDNA instance to the cluster) | `cntk-showcase-logdna` |\n| `logdna_region`       | no  | The region where the existing LogDNA instance has been provisioned. If not provided will default to the cluster `region`. | `us-east` |\n| `provision_sysdig`    | no  | Flag indicating that a new instance of Sysdig should be provisioned. Defaults to `false`      | `true` or `false`          |\n| `sysdig_name`         | no  | The name of the Sysdig instance (If `provision_sysdig` is set to `false` this value is used by the scripts to bind the existing Sysdig instance to the cluster) | `cntk-showcase-sysdig` |\n| `sysdig_region`       | no  | The region where the existing Sysdig instance has been provisioned. If not provided will default to the cluster `region`. | `us-east` |\n| `cos_name`            | no  | The name of the Cloud Object Storage instance that will be used with the OCP cluster. | |\n| `registry_type`       | no  | The type of the Container Registry that will be used with the cluster. Valid values are `icr`, `ocp`, `other`, or `none`. | |\n| `registry_namespace`  | no  | The namespace that should be used in the IBM Container Registry. Defaults to `{resource_group_name}` | `dev-team-one-registry-2020` |\n| `registry_host`       | no  | The host name of the image registry (e.g. us.icr.io or quay.io). This value is only used if the registry_type is set to \"other\" | `quay.io` |\n| `registry_user`       | no  | The username needed to access the image registry. This value is only used if the registry_type is set to \"other\" | `{username}` |\n| `registry_password`   | no  | The password needed to access the image registry. This value is required if the registry_type is set to \"other\". | `{password}` |\n| `source_control_type` | no  | The type of source control system (github, gitlab, or none)                                                      | `github` |\n| `source_control_url`  | no  | The url to the source control system                                                                             | `https://github.com` |\n\n<br />\n\nUpdate `environment-ibmcloud.tfvars` with the appropriate values for your installation. Particularly, be sure to set the \nfollowing values in order to provision a VPC cluster:\n\n- `cluster_exists` to `false`\n- `vpc_cluster` to `true`\n\n### D. (Optional) Customize the installed components\n\n<br />\n\nThe `terraform/stages` directory contains the default set of stages that define the\nmodules that will be applied to the environment. The stages can be customized to change\nthe makeup of the environment that is provisioned by either removing or adding stages from/to the\n`terraform/stages` directory.\n\n**Note:** The stages occasionally have dependencies on other stages (e.g. most all\ndepend on the cluster module, many depend on the namespace module, etc.) so be aware of those\ndependencies as you start making changes. Dependencies are reflected in the `module.{stage name}` references\nin the stage variable list.\n\nThe `terraform/stages/catalog` directory contains some optional\nstages that are prep-configured and can be dropped into the `terraform/stages` directory. Other\nmodules are available from the [Garage Terraform Modules](https://github.com/ibm-garage-cloud/garage-terraform-modules)\ncatalog and can be added as stages to the directory as well. Since this is Terraform,\nany other Terraform scripts and modules can be added to the `terraform/stages` directory\nas desired.\n\n### E. Run the installation\n\n<br />\n\n1. Open a terminal to the `ibm-garage-iteration-zero` directory\n2. Launch a [Developer Tools Docker container](https://github.com/ibm-garage-cloud/ibm-garage-cli-tools \"Cloud Garage Tools Docker image\") from which the Terraform scripts will be run\n   \n    ```shell\n    ./launch.sh\n    ```\n\n    This will download the Cloud Garage Tools Docker image that contains all the necessary tools to execute Terraform scripts\n    and exec shell into the running container. When the container starts it\n    mounts the filesystem's `./terraform/` directory as `/home/devops/src/` and loads the values from the\n    `credentials.properties` file as environment variables.\n\n3. Apply the Terraform by running the provided `runTerraform.sh` script\n\n    ```shell\n    ./runTerraform.sh\n    ```\n\n    This script collects the values provided in the `environment-ibmcloud.tfvars` and the\n    stages defined in the `terraform/stages` to build the Terraform workspace. Along the way it\n    will prompt for a couple pieces of information.\n\n    1. Type of installation: `cluster`\n    \n        This prompt can be skipped by providing `--cluster` as an argument to `./runTerraform.sh`\n\n    2. Handling of an old workspace (if applicable): `keep` or `delete`\n\n        If you executed the script previously for the current cluster configuration and the workspace directory still\n        exists then you will be prompted to either keep or delete the workspace directory. Keep the workspace directory if\n        you want to use the state from the previous run as a starting point to either add or remove configuration. Delete\n        the workspace if you want to start with a clean install of the Toolkit.\n\n        This prompt can be skipped by providing `--delete` or `--keep` as an argument to `./runTerraform.sh`\n\n    3. Verify the installation configuration\n\n        The script will verify some basic settings and prompt if you want to proceed. After you select **Y** (for yes),\n        the Terraform Apply process will begin to create the infrastructure and services for your environment.\n\n        This prompt can be skipped by providing `--auto-approve` as an argument to `./runTerraform.sh`\n\n    Creating a new cluster takes about 1.5 hours on average (but can also take considerably longer)\n    and the rest of the process takes about 30 minutes.\n\n</Tab>\n\n</Tabs>\n\n## Troubleshooting\n\nIf you find that the Terraform provisioning has failed, for Private Catalog delete the workspace and for Iteration Zero  try re-running the `runTerraform.sh` script again.\nThe state will be saved and Terraform will try and apply the configuration to match the desired end state.\n\nIf you find that some of the services have failed to create in the time allocated, try the following with Iteration zero:\n\n1. Manually delete the service instances in your resource group\n3. Re-run the `runTerraform.sh` script with the `--delete` argument to clean up the state\n   \n    ```shell\n    ./runTerraform.sh --delete\n    ```\n","type":"Mdx","contentDigest":"eee8be0df7004b7cf49a0020b3965839","counter":1006,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Provision an IBM Cloud cluster","tabs":["IBM Cloud VPC","IBM Cloud Classic","Multi-Cloud"]},"exports":{},"rawBody":"---\ntitle: Provision an IBM Cloud cluster \ntabs: ['IBM Cloud VPC', 'IBM Cloud Classic', 'Multi-Cloud']\n---\n\nimport Globals from 'gatsby-theme-carbon/src/templates/Globals';\n\n<PageDescription>\n\nSteps to provision a new <Globals name=\"ic\" />-managed cluster\nrunning on VPC infrastructure using the <Globals name=\"shortName\" />.\n\n</PageDescription>\n\nSee the following for instructions on how to\nprovision [new <Globals name=\"ic\" />-managed clusters running on classic\ninfrastructure](./ibm-cloud-on-classic)\n\n<Tabs>\n\n<Tab label=\"Private Catalog\" open=\"true\">\n\n<InlineNotification>\n\nThese steps assume the private catalog has been created and populated with the <Globals name=\"shortName\" /> tiles during \nthe [prepare the account](../prepare-account#3.-create-the-private-catalog) steps.\n\n</InlineNotification>\n\n<br />\n\n1. Log in to the <Globals name=\"ic\" /> Console.\n2. Select **Catalog** from the top menu.\n3. From the side menu, select your catalog from the drop-down list (e.g. `Team Catalog`). (**IBM Cloud catalog** should be selected initially.)\n4. Click **Private** on the side menu to see the private catalog entries\n5. Click on the **220. Cloud-Native VPC cluster** tile\n6. Enter values for the variables list provided.\n\n    | **Variable**          | **Description**                                                                               | **eg. Value**                 |\n    |-----------------------|-----------------------------------------------------------------------------------------------|-------------------------------|\n    | `ibmcloud_api_key`    | The API key from IBM Cloud Console that has ClusterAdmin access and supports service creation | `{guid API key from Console}` |\n    | `resource_group_name` | The existing resource group in the account where the cluster will be created                  | `dev-team-one`                |\n    | `region`              | The region where the cluster will be provisioned.                                             | `us-east`, `eu-gb`, etc       |\n    | `cluster_name`        | The name of the cluster that will be provisioned.                                             | `dev-team-one-iks-117-vpc`    |\n    | `vpc_zone_names`      | A comma-separated list of the VPC zones that should be used for worker nodes.                 | `us-south-1` or `us-east-1,us-east-2` |\n    | `cluster_type`        | The type of cluster into which the toolkit will be installed. The default is `OpenShift 4.5`. | `kubernetes`, `ocp3`, `ocp4`, `ocp44`, or `ocp45` |\n    | `flavor`              | The flavor of machine that should be provisioned for each worker. Defaults to `mx2.4x32`.     | `mx2.4x32`          |\n    | `cluster_worker_count`| The number of worker nodes that should be provisioned for each zone. Defaults to `3`          | `3` |\n    | `cluster_provision_cos`| Flag indicating that a new Object Storage instance should be provisioned. Defaults to `true` | `true` or `false`          |\n    | `cos_name`            | The name of the Object Storage instance (If `cluster_provision_cose` is set to `true` this value is required | `cntk-showcase-cos` |\n\n7. Check the box to accept the **Apache 2** license for the tile.\n8. Click **Install** to start the install process\n\nThis will kick off the installation of the <Globals name=\"shortName\" /> using an\nIBM Cloud Private Catalog Tile. The progress can be reviewed from the\n**Schematics** entry\n\n</Tab>\n\n<Tab label=\"Iteration Zero\">\n\n### A. Download the Iteration Zero scripts\n\n<br />\n\n1. Clone the [ibm-garage-iteration-zero](https://github.com/ibm-garage-cloud/ibm-garage-iteration-zero) Git repository to your local filesystem\n   \n    ```shell\n    git clone git@github.com:ibm-garage-cloud/ibm-garage-iteration-zero.git\n    ```\n\n### B. Configure the credentials\n\n<br />\n\n1. In a terminal, change to the `ibm-garage-iteration-zero` cloned directory\n   \n    ```shell\n    cd ibm-garage-iteration-zero\n    ```\n\n2. Copy the `credentials.template` file to a file named `credentials.properties`\n   \n    ```shell\n    cp credentials.template credentials.properties\n    ```\n\n   **Note:** `credentials.properties` is already listed in `.gitignore` to prevent the\n   private credentials from being committed to the git repository\n\n3. Update the value for the `ibmcloud.api.key` property in `credentials.properties` with your <Globals name=\"ic\" /> API key\n\n    **Note:** The API key should have been set up during [plan installation](/getting-started-0/prepare-account).\n\n### C. Configure the environment variables\n\n<br />\n\nThe settings for creating the <Globals name=\"shortName\" /> on <Globals name=\"ic\" /> are set in the `environment-ibmcloud.tfvars`\nfile in the `./terraform/settings` directory of the `ibm-garage-iteration-zero` repository.\n\nThere are a number of values that can be applied in the file, some required and some optional. Consult with\nthe following table to determine which values should be used:\n\n| **Variable**          | **Required?** | **Description**                                                                     | **eg. Value**                 |\n|-----------------------|-----|-----------------------------------------------------------------------------------------------|-------------------------------|\n| `cluster_type`        | yes | The type of cluster into which the toolkit will be installed                                  | `kubernetes`, `ocp3`, `ocp4` or `ocp44` |\n| `cluster_exists`      | yes | Flag indicating if the cluster already exists. (`false` means the cluster should be provisioned) | `false`          |\n| `vpc_cluster`         | yes | Flag indicating that the cluster has been built on VPC infrastructure. Defaults to `true`     | `true`          |\n| `name_prefix`         | no  | The prefix that should be applied for any resources that are provisioned. Defaults to `{resource_group_name}` | `dev-one`     |\n| `cluster_name`        | no  | The name of the cluster (If `cluster_exists` is set to `true` then this name should match an existing cluster). Defaults to `{prefix_name}-cluster` or `{resource_group_name}-cluster` | `dev-team-one-iks-117-vpc` |\n| `resource_group_name` | yes | Existing resource group in the account where the cluster has been created                     | `dev-team-one`                |\n| `region`              | yes | The region where the cluster has been/will be provisioned                                     | `us-east`, `eu-gb`, etc       |\n| `vpc_zone_names`      | no  | A comma-separated list of the VPC zones that should be used for worker nodes. This value is required if `cluster_exists` is set to `false` and `vpc_cluster` is set to `true` | `us-south-1` or `us-east-1,us-east-2` |\n| `provision_logdna`    | no  | Flag indicating that a new instance of LogDNA should be provisioned. Defaults to `false`      | `true` or `false`          |\n| `logdna_name`         | no  | The name of the LogDNA instance (If `provision_logdna` is set to `false` this value is used by the scripts to bind the existing LogDNA instance to the cluster) | `cntk-showcase-logdna` |\n| `logdna_region`       | no  | The region where the existing LogDNA instance has been provisioned. If not provided will default to the cluster `region`. | `us-east` |\n| `provision_sysdig`    | no  | Flag indicating that a new instance of Sysdig should be provisioned. Defaults to `false`      | `true` or `false`          |\n| `sysdig_name`         | no  | The name of the Sysdig instance (If `provision_sysdig` is set to `false` this value is used by the scripts to bind the existing Sysdig instance to the cluster) | `cntk-showcase-sysdig` |\n| `sysdig_region`       | no  | The region where the existing Sysdig instance has been provisioned. If not provided will default to the cluster `region`. | `us-east` |\n| `cos_name`            | no  | The name of the Cloud Object Storage instance that will be used with the OCP cluster. | |\n| `registry_type`       | no  | The type of the Container Registry that will be used with the cluster. Valid values are `icr`, `ocp`, `other`, or `none`. | |\n| `registry_namespace`  | no  | The namespace that should be used in the IBM Container Registry. Defaults to `{resource_group_name}` | `dev-team-one-registry-2020` |\n| `registry_host`       | no  | The host name of the image registry (e.g. us.icr.io or quay.io). This value is only used if the registry_type is set to \"other\" | `quay.io` |\n| `registry_user`       | no  | The username needed to access the image registry. This value is only used if the registry_type is set to \"other\" | `{username}` |\n| `registry_password`   | no  | The password needed to access the image registry. This value is required if the registry_type is set to \"other\". | `{password}` |\n| `source_control_type` | no  | The type of source control system (github, gitlab, or none)                                                      | `github` |\n| `source_control_url`  | no  | The url to the source control system                                                                             | `https://github.com` |\n\n<br />\n\nUpdate `environment-ibmcloud.tfvars` with the appropriate values for your installation. Particularly, be sure to set the \nfollowing values in order to provision a VPC cluster:\n\n- `cluster_exists` to `false`\n- `vpc_cluster` to `true`\n\n### D. (Optional) Customize the installed components\n\n<br />\n\nThe `terraform/stages` directory contains the default set of stages that define the\nmodules that will be applied to the environment. The stages can be customized to change\nthe makeup of the environment that is provisioned by either removing or adding stages from/to the\n`terraform/stages` directory.\n\n**Note:** The stages occasionally have dependencies on other stages (e.g. most all\ndepend on the cluster module, many depend on the namespace module, etc.) so be aware of those\ndependencies as you start making changes. Dependencies are reflected in the `module.{stage name}` references\nin the stage variable list.\n\nThe `terraform/stages/catalog` directory contains some optional\nstages that are prep-configured and can be dropped into the `terraform/stages` directory. Other\nmodules are available from the [Garage Terraform Modules](https://github.com/ibm-garage-cloud/garage-terraform-modules)\ncatalog and can be added as stages to the directory as well. Since this is Terraform,\nany other Terraform scripts and modules can be added to the `terraform/stages` directory\nas desired.\n\n### E. Run the installation\n\n<br />\n\n1. Open a terminal to the `ibm-garage-iteration-zero` directory\n2. Launch a [Developer Tools Docker container](https://github.com/ibm-garage-cloud/ibm-garage-cli-tools \"Cloud Garage Tools Docker image\") from which the Terraform scripts will be run\n   \n    ```shell\n    ./launch.sh\n    ```\n\n    This will download the Cloud Garage Tools Docker image that contains all the necessary tools to execute Terraform scripts\n    and exec shell into the running container. When the container starts it\n    mounts the filesystem's `./terraform/` directory as `/home/devops/src/` and loads the values from the\n    `credentials.properties` file as environment variables.\n\n3. Apply the Terraform by running the provided `runTerraform.sh` script\n\n    ```shell\n    ./runTerraform.sh\n    ```\n\n    This script collects the values provided in the `environment-ibmcloud.tfvars` and the\n    stages defined in the `terraform/stages` to build the Terraform workspace. Along the way it\n    will prompt for a couple pieces of information.\n\n    1. Type of installation: `cluster`\n    \n        This prompt can be skipped by providing `--cluster` as an argument to `./runTerraform.sh`\n\n    2. Handling of an old workspace (if applicable): `keep` or `delete`\n\n        If you executed the script previously for the current cluster configuration and the workspace directory still\n        exists then you will be prompted to either keep or delete the workspace directory. Keep the workspace directory if\n        you want to use the state from the previous run as a starting point to either add or remove configuration. Delete\n        the workspace if you want to start with a clean install of the Toolkit.\n\n        This prompt can be skipped by providing `--delete` or `--keep` as an argument to `./runTerraform.sh`\n\n    3. Verify the installation configuration\n\n        The script will verify some basic settings and prompt if you want to proceed. After you select **Y** (for yes),\n        the Terraform Apply process will begin to create the infrastructure and services for your environment.\n\n        This prompt can be skipped by providing `--auto-approve` as an argument to `./runTerraform.sh`\n\n    Creating a new cluster takes about 1.5 hours on average (but can also take considerably longer)\n    and the rest of the process takes about 30 minutes.\n\n</Tab>\n\n</Tabs>\n\n## Troubleshooting\n\nIf you find that the Terraform provisioning has failed, for Private Catalog delete the workspace and for Iteration Zero  try re-running the `runTerraform.sh` script again.\nThe state will be saved and Terraform will try and apply the configuration to match the desired end state.\n\nIf you find that some of the services have failed to create in the time allocated, try the following with Iteration zero:\n\n1. Manually delete the service instances in your resource group\n3. Re-run the `runTerraform.sh` script with the `--delete` argument to clean up the state\n   \n    ```shell\n    ./runTerraform.sh --delete\n    ```\n","fileAbsolutePath":"/home/runner/work/ibm-garage-developer-guide/ibm-garage-developer-guide/src/pages/getting-started-day-0/provision-cluster/ibm-cloud-vpc.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3037994772","3037994772","3273249464","530240012","530240012","768070550"]}